
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parallel Computing &#8212; Practical Data Science</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Distributed Computing with dask" href="distributed_computing.html" />
    <link rel="prev" title="Solving Performance Issues" href="performance_solutions.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Parallel-Computing">
<h1>Parallel Computing<a class="headerlink" href="#Parallel-Computing" title="Permalink to this headline">¶</a></h1>
<p>Before you dive into this, let me just tell you the punchline of this entire page right up front: parallelism is the <strong>last tool</strong> you want to turn to for speed. It is <strong>not</strong> a silver bullet, it will generally take you <em>significant</em> time to implement, the speed improvements from parallelism are generally <strong>much</strong> smaller than what you get from other performance improvement methods (see <a class="reference internal" href="performance_understanding.html"><span class="doc">Understanding Performance</span></a> and <a class="reference internal" href="performance_solutions.html"><span class="doc">Performance
Solutions</span></a>), and the headaches of parallelizing code are many.</p>
<div class="section" id="What-is-Parallelism">
<h2>What <em>is</em> Parallelism<a class="headerlink" href="#What-is-Parallelism" title="Permalink to this headline">¶</a></h2>
<p>Parallelism is the process of:</p>
<ol class="arabic simple">
<li><p>taking a single problem,</p></li>
<li><p>breaking it into lots of smaller problems,</p></li>
<li><p>assigning those smaller problems to a number of processing cores that are able to operate independently, and</p></li>
<li><p>recombining the results.</p></li>
</ol>
<p>As this list shows, parallelism is not easy, and so not only does it take substantial developer time (the time it takes you to implement it), but there are computer-time costs to breaking down problems, distributing them, and recombining them, often limiting the returns you will see to parallelism.</p>
</div>
<div class="section" id="Why-is-Paralleism-important?">
<h2>Why is Paralleism important?<a class="headerlink" href="#Why-is-Paralleism-important?" title="Permalink to this headline">¶</a></h2>
<p>Given all that, why is parallelism all the rage?</p>
<p>The simple answer is that, when it comes to running a serial program (a problem where you run your code in sequence, one step at a time), processors stop getting faster in about the mid-2000s.</p>
<p>This might surprise you. Most of us have heard that Moore’s Law dictates that processors are doubling in performance every 18 months. The reality, however, is more complicated.</p>
<p>Moore’s law used to apply to a number of aspect of processors: the size of transistors, the number of transisters, and the speed that a processor executed serial code. But as shown in the figure below, processor frequency and the speed of serial execution stopped this pattern of doubling in the mid-2000s (serial execution has still been making small gains since then, but even that is iffy – those improvements are due to little hacks that only work when programs work in just the right way).</p>
<p><img alt="42-years-processor-trend" src="_images/42-years-processor-trend.png" /></p>
<p>Source: <a class="reference external" href="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/">karlrupp.net</a></p>
<p>And so since chip makers have lost the ability to make their processors faster, they’ve turned to just giving us more and more processors in the form or more “cores”: components of chips that are capable of independent operation. And leveraging the availability of massive numbers of cores (either in single machines, or for huge datasets, across lots of computers) has become the only place left to go for performance!</p>
</div>
<div class="section" id="Theoretical-Limits-of-Parallelism">
<h2>Theoretical Limits of Parallelism<a class="headerlink" href="#Theoretical-Limits-of-Parallelism" title="Permalink to this headline">¶</a></h2>
<p>OK, now let’s talk about the theoretical limits of parallelism.</p>
<p>The biggest problem with parallelism is that it’s very hard to break some problems into smaller pieces you can work on simultaneously. Somethings you do on computers are fundamentally <em>serial</em> / <em>sequential</em>, and thus cannot be broken up.</p>
<p>For example, suppose you want to simulate how weather evolves over time. The only way to simulate how weather will evolve on day 2 of your simulation is to wait till you’ve finished simulating day 1 so you can use those results as the starting point for your simulation in day 2. That means there’s no way to fully parallelize a weather simulation, since the results at time <span class="math notranslate nohighlight">\(t\)</span> will always depend on the results generated at time <span class="math notranslate nohighlight">\(t-1\)</span>.</p>
<p>To be clear, this is not to say there are <em>no</em> opportunities to speed up weather simulations – since you generally run weather simulations over and over and then look at the <em>average</em> prediction, each separate simulation can be parallelized. But it does mean that <em>even if you had infinite processors</em>, you couldn’t bring the time it takes to simulate the weather down to zero because you’d have to finish simulating day 1 before you can simulate day 2.</p>
<div class="section" id="Amdahl’s-Law">
<h3>Amdahl’s Law<a class="headerlink" href="#Amdahl’s-Law" title="Permalink to this headline">¶</a></h3>
<p>The formalization of this idea is what’s called Amdahl’s Law, which gives the <em>theoretical</em> limits of parallelization. If <span class="math notranslate nohighlight">\(P\)</span> is the proportion of your algorithm that can be parallelized, and <span class="math notranslate nohighlight">\(N\)</span> is the number of cores available, the fundamental limit to the speed-up you can get from parallelization is given by:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{(1-P)+\frac{P}{N}}\]</div>
<p>Which also means that even with infinite cores, you can’t get past the speedup limit of:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{(1-P)}\]</div>
<p>Expressed graphically, this is:</p>
<p><img alt="amdahlslaw" src="_images/amdahlslaw.png" /></p>
<p>Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Wikipedia</a></p>
<p>As this figure shows, even for a task that is <em>95% parallelizable</em>, the biggest possible performance gain you will ever get (with infinite cores), even ignoring any real-world overheads required to execute that parallelization is 20x.</p>
<p>Now, to be clear, that doesn’t mean there aren’t situations where the best strategy is parallelism. If you’ve exhausted all your other opportunities to speed up your code, parallelism may be all that’s left. And in data science, it’s not uncommon to have code that can be much more than 95% parallelizable – for example, if you need to run a simulation 1,000,000 times, and each run is relatively short, you can get close to 100% parallelizable. But the point is that parallelism is no silver bullet,
and it’s important to think hard about whether your problem is suited to parallelizing before you invest in trying to parallelize your code!</p>
</div>
</div>
<div class="section" id="Vocabulary">
<h2>Vocabulary<a class="headerlink" href="#Vocabulary" title="Permalink to this headline">¶</a></h2>
<p>The terms processor and core are often used interchangable among computer scientists to refer to “units capable of indepedent operation”. This can be confusing because most of us think of a “processor” as a single square piece of silicone material that is plugged into our motherboard (e.g. a Intel i9, or an Intel i7):</p>
<p><img alt="intel_chip" src="_images/intel_chip.jpg" /></p>
<p>But what most of us think of as processors today often have many cores (the Intel i9 on my laptop has 8 cores, each capable of working relatively independently). Here’s a labeled image of the inside of a core i7 processor with four distinct “cores”:</p>
<p><img alt="core_i7" src="_images/core_i7.gif" /></p>
<p>And while most of us refer to the chip as a processor, and these cores as… cores, computer scientists have a tendency to use the term “processors” and “core” interchangeable to refer to what most of us call “cores”.</p>
<p>And the question of “how many processors / cores do I have” gets even more confusing for two reasons:</p>
<p>First, many desktops have two seperate chips (i.e. two Intel i7s, or two Intel i9s), leading some people to call these multi-processor machines. :/</p>
<p>Second, many modern processors have a feature called “hyperthreading”, which is where two tasks can be assigned to the same individual core. That core can’t do both tasks at the same time, but it can try and switch between them efficiently. For example, if the first task stalls out while it’s waiting to get more data from memory (remember how <a class="reference internal" href="performance_understanding.html"><span class="doc">slow memory is</span></a>), the core can switch to working on the second task instead of just sitting around. This <em>can</em> offer
additional performance, but much less than actually having an actual additional independent core, and actual performance depends on the nature of the job being run (I’ve heard data science people say if two cores gets you 2x performance, then hyperthreading gets you about 1.25x performance, which seems roughly consistent with my experience. But again this depends <em>hugely</em> on the nature of the job you’re parallelizing).</p>
<p>Confusingly, though, the way hyperthreading manifests is by telling your operating system that you have two cores for every physical core on your computer. So for example, while my computer has only 8 physical cores, if I open Python and run:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
16
</pre></div></div>
</div>
<p>I get 16, because each physical core is presenting itself as two cores to my operating system.</p>
<p>So, how you you think about this?</p>
<ul class="simple">
<li><p>In most contexts (i.e. if you’re not shoping for hardware), expect the terms “cores” and “processors” to both be used to refer to independent processing cores.</p></li>
<li><p>Don’t worry about whether they’re distributed across two different physical chips or not – all that matters is the number of cores on your computer.</p></li>
<li><p>On most computers, expect the operating system to think that you have twice as many cores (sometimes referred to as “logical cores”) as you have actual “physical cores”, but expect your performance to reflect the number of physical cores you actually have.</p></li>
</ul>
</div>
<div class="section" id="An-Example:-Parallel-Processing-with-Joblib">
<h2>An Example: Parallel Processing with Joblib<a class="headerlink" href="#An-Example:-Parallel-Processing-with-Joblib" title="Permalink to this headline">¶</a></h2>
<p>We won’t get into really sophisticated parallel processing in this tuturial – writing parallelized code is a discipline unto itself – be we can do a little “embarassingly parallel” computing.</p>
<p>An “embarassingly parallel” task is one where each component of your parallel job is entirely independent of every other part. For example, consider the weather simulation we discussed above – while Day 2 is not independent of Day 1, each attempt to simulate 7 days of weather <em>is</em> fully independent from every other run, making it embarassingly parallel.</p>
<p>So let’s develop an embarassingly parallel simulation, and run it in parallel using the <code class="docutils literal notranslate"><span class="pre">Joblib</span></code> library (FYI: R has an analogous library called <a class="reference external" href="https://www.rdocumentation.org/packages/foreach/versions/1.4.7/topics/foreach">foreach</a>).</p>
<p>First, let’s develop a dumb little weather simulation with a central tendency so the temperature tomorrow is the function of some anchor temperature (let’s say 72 degrees farenheit) and the realized temperature today (so if it’s hot today, it’s likely to be hot tomorrow, but if it’s above the long-term average of 72, it’s also likely to be closer to 72 than the temp today).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>

<span class="k">def</span> <span class="nf">weather_at_t_plus_one</span><span class="p">(</span><span class="n">temp_at_t</span><span class="p">,</span> <span class="n">std_dev</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">72</span> <span class="o">+</span> <span class="n">temp_at_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">std_dev</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simulate_weather</span><span class="p">(</span><span class="n">starting_temp</span><span class="p">,</span> <span class="n">weather_std_dev</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">weather_at_time_t</span> <span class="o">=</span> <span class="n">starting_temp</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">weather_at_time_t</span> <span class="o">=</span> <span class="n">weather_at_t_plus_one</span><span class="p">(</span><span class="n">weather_at_time_t</span><span class="p">,</span> <span class="n">weather_std_dev</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weather_at_time_t</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">simulate_weather</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
82.56669873391075
</pre></div></div>
</div>
<p>Now suppose we want to see what the weather is likely to be like in 10 days. Even if this simulation were realistic, we wouldn’t be able to answer that question by just running it once, since every time we run the simulation we get different data. Instead, we want to run the simulation over and over and look at the distribution of outcomes.</p>
<p>The simpliest way to do this would be with a little for-loop:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simulate_weather</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 21.5 s, sys: 68.5 ms, total: 21.5 s
Wall time: 21.3 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
72.0096841595076
</pre></div></div>
</div>
<p>But look how slow this is! If we want to run this 1,000,000 times, it takes a full 24 seconds on average!</p>
<p>So let’s try parallelizing these simulations using <code class="docutils literal notranslate"><span class="pre">joblib</span></code>. (Make sure to install joblib if you haven’t yet with <code class="docutils literal notranslate"><span class="pre">conda</span></code> or <code class="docutils literal notranslate"><span class="pre">pip</span></code>!)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span> <span class="c1">#Import the relevant tools</span>

<span class="c1"># Joblib wants a function that only takes a single argument.</span>
<span class="c1"># Since we&#39;re not changing our parameters with each run, we just create a</span>
<span class="c1"># &quot;little barrier function&quot; that takes one argument.</span>

<span class="k">def</span> <span class="nf">run_simulation</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">simulate_weather</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">run_simulation</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
72.00584856772745
</pre></div></div>
</div>
<p>And that’s it!</p>
<p>OK, so that line of code is kinda inscrutible, so let’s unpack it:</p>
<p><code class="docutils literal notranslate"><span class="pre">Parallel(n_jobs=10)</span></code>: this is where you specify how many processes you want to spin up. You don’t want to parallelize to more logical cores than are available on your computer (generally, two times the number of physical cores on your computer if you have an Intel chip). If you try and parallelize to 4 cores but your computer only has 2, then your operating system will just keep forcing your cores to stop and switch from running one pair of processes to running the other pair, back and forth,
and that switching takes time. To see how many cores you have, run:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
16
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">delayed(run_simulation)(i)</span> <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(1000000)</span></code>: This specifies what you want executed on all these parallel processes. In this case, we’re saying “we want to execute the function <code class="docutils literal notranslate"><span class="pre">run_simulation</span></code> with the argument <code class="docutils literal notranslate"><span class="pre">i</span></code> for all values of <code class="docutils literal notranslate"><span class="pre">i</span></code> from 0 to 1,000,000.</p>
<p>So that’s all you have to type, but that kinda obscures what all is happening.</p>
<p>When you run this code, here’s what happens:</p>
<ol class="arabic simple">
<li><p>Joblib starts up as many new Python sessions as you hnave specified in <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>. These are fully independent, just as though you had openned a bunch of new terminal windows and typed Python into each one.</p></li>
<li><p>Joblib then passes each of these separate processes the function (in this case <code class="docutils literal notranslate"><span class="pre">run_simulation</span></code>) you want to run. Remember because each process is it’s own unique and beautiful snowflake, it doesn’t know what your current Python session knows! Joblib will also pass data to these processes if your job uses data. That means that if, instead of just an index like <code class="docutils literal notranslate"><span class="pre">i</span></code>, you were passing a dataframe in as an argument, Joblib would make a new copy of your dataframe in everyone one of these
processes (which means if your dataframe takes up 30% of your RAM, and you try and parallelize over 8 processes, you’re run out of memory when joblib makes 8 copies!).</p></li>
<li><p>Joblib then starts telling these processes to run your function (<code class="docutils literal notranslate"><span class="pre">run_simulation</span></code>) for different values of <code class="docutils literal notranslate"><span class="pre">i</span></code>. It takes care of making sure that all your <code class="docutils literal notranslate"><span class="pre">i</span></code> values run somewhere.</p></li>
<li><p>Then when each process finishes running <code class="docutils literal notranslate"><span class="pre">run_simulation(i)</span></code>, it passes the results back to joblib in your primary process, and puts them in a list.</p></li>
<li><p>Then when all values of <code class="docutils literal notranslate"><span class="pre">i</span></code> have been used, <code class="docutils literal notranslate"><span class="pre">joblib</span></code> closes all those processes.</p></li>
</ol>
<p>So, how much speed benefit does this get us? Previously we saw that running our simulation 1 million times took approximately 24 seconds to run. Let’s see how it looks with a 10 core parallelization (note my computer has 8 physical cores, so we’re using a little hyperthreading. I’m just doing 10 since the math is easy. :)).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">run_simulation</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 207 ms, total: 15.3 s
Wall time: 15.5 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
72.00663479045383
</pre></div></div>
</div>
<p>15 seconds!</p>
<p>Yeah…</p>
<p>So this is what I mean about parallel processing: there’s a lot of fixed costs to getting this independent processes running and passing things back and forth, as a result of which the benefits of parallelism are almost always sub-linear (i.e. using 10 processors instead of 1 will result in &lt;10x speedups).</p>
<p>Now to be clear, the benefits won’t always be so small. Since the “cost” of setting up these processes is pretty constant, the longer each simulation runs, the larger the gains. For example, let’s compare performance if we simulate for 100 days instead of 10 (so there are more actual simulation steps taking place for each message passed back and forth between processes).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simulate_weather</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 3min 30s, sys: 102 ms, total: 3min 30s
Wall time: 3min 30s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="k">def</span> <span class="nf">run_simulation</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">simulate_weather</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">run_simulation</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 30.7 s, sys: 613 ms, total: 31.3 s
Wall time: 45.9 s
</pre></div></div>
</div>
<p>Much better. From 180 seconds to 31 seconds. But still that’s only a 5x speed up with 10 cores!</p>
<p>By the way, if you want to see the parallelism in action, you can look at your Activity Monitor (mac) or Resource Monitor (Windows). Here’s mine while that is running:</p>
<p><img alt="activity_monitor_parallelism" src="_images/activity_monitor_parallelism.png" /></p>
<p>But at the risk of beating a dead horse, we can get far better performance benefits just by using the tips in the <a class="reference internal" href="performance_understanding.html"><span class="doc">performance section</span></a> like vectorizing:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">sims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">std_dev</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">sims</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">72</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">sims</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">sims</span><span class="p">[:,</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">72</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 6.21 s, sys: 253 ms, total: 6.47 s
Wall time: 6.47 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sims</span><span class="p">[:,</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
71.98881644218754
</pre></div></div>
</div>
<p>THAT only took 6 seconds…</p>
</div>
<div class="section" id="Multi-processing-versus-Multi-threading">
<h2>Multi-processing versus Multi-threading<a class="headerlink" href="#Multi-processing-versus-Multi-threading" title="Permalink to this headline">¶</a></h2>
<p>This type of parallelism – where we create entirely new processes to run our code, and we have to copy over our data to each new process – is what’s called “multiprocessing”. Multiprocessing is nice because (a) you can use it on big server clusters where different processes may be running on different computers, and (b) it’s quite safe. As a result, <em>most</em> parallelism you’re like to encounter will be multi-processing.</p>
<p>However, there is another type of parallelism to be aware of called multi-threading. In multi-threading, all the code being run (each sequence execution of which is called a “thread”) exists within a single process, meaning that all the different threads have access to the same objects in memory. This massively reduces duplication because you don’t have to copy your code and data and pass it around – all the threads can see the same parts of memory.</p>
<p>But multi-threading has three major shortcomings:</p>
<ul class="simple">
<li><p>It is <em>very</em> easy to run into very subtle but profound problems that lead to corruption (the biggest of which is something called <a class="reference external" href="https://youtu.be/7ENFeb-J75k">Race Conditions</a>).</p></li>
<li><p>Multi-threading can only distribute a job over the cores <em>on a single computer</em>, meaning it can’t be used to distribute a job over hundreds of cores in a large computing cluster.</p></li>
<li><p>You generally can’t use multi-thread parallelism in Python because of a <a class="reference external" href="https://realpython.com/python-gil">fundamental component of its architecture (the GIL)</a>.</p></li>
</ul>
<p>So… multi-threading isn’t something you’re like likely to try and implement yourself. But if you <em>do</em> need to get into multi-threading, consider jumping to a language like Julia, which allows for <a class="reference external" href="https://julialang.org/blog/2019/07/multithreading">easy multi-threading, complete with security features.</a></p>
<p>But that isn’t to say you may not <em>benefit</em> from multi-threading. More and more tools are implementing parallelism (even Python packages, by building in the multi-threading into the C code that underlies that package you use in Python), so you may just get some benefits for free!</p>
</div>
<div class="section" id="GPU-Parallelism">
<h2>GPU Parallelism<a class="headerlink" href="#GPU-Parallelism" title="Permalink to this headline">¶</a></h2>
<p>Something related to general parallelism is GPU parallelism. GPU parallelism is the practice of using Graphical Processing Units (GPUs) to do <em>extremely</em> parallel computing.</p>
<p>GPUs are basically computers onto themselves designed for the sole purpose of, as the name suggests, processing graphics. As it turns out, however, most of what one does when processing graphics is <em>lots</em> of matrix algebra. And so in recent years researchers, have started using GPUs for scientific research.</p>
<p>GPU processors aren’t “general purpose” processors – they only do a few things, and as a result of that specialization, what they do they do <em>fast</em>. Moreover, they are massively parallel: while your CPU (your Intel chip) may have ~4-8 cores, a modern GPU has either hundreds or thousands of cores.</p>
<p>But because GPUs are basically computers onto themselves, to use them you have to write special code to both manage all those cores and also manage the movement of data from your regular computer to the GPU. All of which is to say: you probably won’t write your own GPU parallel algorithm, but if you end up in an area that uses them a lot (i.e. training neural networks), the libraries you use will.</p>
</div>
<div class="section" id="Parallelism-and-Distributed-Computing">
<h2>Parallelism and Distributed Computing<a class="headerlink" href="#Parallelism-and-Distributed-Computing" title="Permalink to this headline">¶</a></h2>
<p>In this reading, we’ve mostly focused on discussing the <em>principles</em> of parallelism – the goal, the fundamental problems, and some basic examples of parallelism.</p>
<p>In a future class, we’ll discuss some packages that are designed to make parallelism as easy as possible in situations where you really need parallelism. In particular, we’ll focus on tools for <em>distributed computer</em>: situations where you don’t just want to parallelize across the cores in your computer, but across many computers (e.g. in the cloud). But when we get to these tools – like <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> and <code class="docutils literal notranslate"><span class="pre">dask</span></code> – try to keep in mind the lessons we’ve learned here, because everything you’ve
learned here applies to those packages as well!</p>
</div>
<div class="section" id="Want-to-Learn-More?">
<h2>Want to Learn More?<a class="headerlink" href="#Want-to-Learn-More?" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.youtube.com/watch?v=zX4ZNfvw1cw">Check out this terrific talk by Sophie Wilson!</a></p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Practical DS</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="class_schedule.html">CLASS SCHEDULE</a></li>
</ul>
<p class="caption"><span class="caption-text">PYTHON &amp; PANDAS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="setup_environment.html">Setting Up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="managing_python_packages.html">Managing Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_v_r.html">Python / R Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="vars_v_objects.html">Python: Vars v Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="ints_and_floats.html">Numbers in Computers</a></li>
<li class="toctree-l1"><a class="reference internal" href="pandas_series.html">Pandas 1: Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="pandas_dataframes.html">Pandas 2: DataFrames</a></li>
<li class="toctree-l1"><a class="reference internal" href="plotting_part1.html">Plotting, Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="plotting_part2.html">Plotting, Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="views_and_copies_in_pandas.html">Pandas 3: Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="cleaning_editingvalues.html">Cleaning: Editing Values</a></li>
</ul>
<p class="caption"><span class="caption-text">OTHER TOOLS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="command_line_part1.html">Command Line, Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="command_line_part2.html">Command Line, Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="git_and_github.html">Git and Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="parquet.html">Parquet Format</a></li>
</ul>
<p class="caption"><span class="caption-text">CLOUD</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_what_is_it.html">What Is The Cloud?</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_azureml.html">AzureML</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_dask.html">Set Up Dask on AzureML</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_azurestorage.html">More on Azure Storage</a></li>
</ul>
<p class="caption"><span class="caption-text">SKILLS</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_help.html">Getting Help Online</a></li>
<li class="toctree-l1"><a class="reference internal" href="defensive_programming.html">Defensive Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow.html">Workflow Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="what_is_big_data.html">What is Big Data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="big_data_strategies.html">Working with Big Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_understanding.html">Understanding Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_solutions.html">Solving Performance Probs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Parallel Computing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#What-is-Parallelism">What <em>is</em> Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Why-is-Paralleism-important?">Why is Paralleism important?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Theoretical-Limits-of-Parallelism">Theoretical Limits of Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Vocabulary">Vocabulary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#An-Example:-Parallel-Processing-with-Joblib">An Example: Parallel Processing with Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Multi-processing-versus-Multi-threading">Multi-processing versus Multi-threading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#GPU-Parallelism">GPU Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Parallelism-and-Distributed-Computing">Parallelism and Distributed Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Want-to-Learn-More?">Want to Learn More?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_computing.html">Distributed Computing</a></li>
</ul>
<p class="caption"><span class="caption-text">OTHER</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="not_a_mids_student.html">Not a MIDS Student?</a></li>
<li class="toctree-l1"><a class="reference internal" href="cheatsheets.html">Cheat Sheets</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="performance_solutions.html" title="previous chapter">Solving Performance Issues</a></li>
      <li>Next: <a href="distributed_computing.html" title="next chapter">Distributed Computing with dask</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Nick Eubank.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/parallelism.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-133829453-1']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>